//---------------------------------------------------------------------------
//    $Id: program.cc 56 2015-02-06 13:05:10Z kronbichler $
//    Version: $Name$
//
//    Copyright (C) 2013 - 2015 by Katharina Kormann and Martin Kronbichler
//
//---------------------------------------------------------------------------

#include <deal.II/base/convergence_table.h>
#include <deal.II/base/function.h>
#include <deal.II/base/logstream.h>
#include <deal.II/base/quadrature_lib.h>
#include <deal.II/base/timer.h>

#include <deal.II/lac/full_matrix.h>
#include <deal.II/lac/parallel_vector.h>
#include <deal.II/lac/precondition.h>
#include <deal.II/lac/solver_cg.h>
#include <deal.II/lac/solver_gmres.h>
#include <deal.II/lac/trilinos_sparse_matrix.h>

#include <deal.II/fe/fe_dgq.h>
#include <deal.II/fe/fe_values.h>
#include <deal.II/fe/mapping_q.h>

#include <deal.II/distributed/tria.h>
#include <deal.II/grid/grid_generator.h>
#include <deal.II/grid/tria.h>
#include <deal.II/grid/tria_accessor.h>
#include <deal.II/grid/tria_boundary_lib.h>
#include <deal.II/grid/tria_iterator.h>

#include <deal.II/multigrid/mg_coarse.h>
#include <deal.II/multigrid/mg_matrix.h>
#include <deal.II/multigrid/mg_smoother.h>
#include <deal.II/multigrid/mg_tools.h>
#include <deal.II/multigrid/mg_transfer_matrix_free.h>
#include <deal.II/multigrid/multigrid.h>

#include <deal.II/numerics/data_out.h>
#include <deal.II/numerics/vector_tools.h>

#include <deal.II/matrix_free/fe_evaluation.h>
#include <deal.II/matrix_free/matrix_free.h>

#include <deal.II/meshworker/assembler.h>
#include <deal.II/meshworker/dof_info.h>
#include <deal.II/meshworker/integration_info.h>
#include <deal.II/meshworker/loop.h>

#include <deal.II/integrators/laplace.h>

//#define USE_FEQ

#include <fstream>
#include <sstream>

#include "solution.h"

namespace Step37
{
using namespace dealii;

const unsigned int degree_finite_element = 6;
const unsigned int dimension = 3;
const bool run_variable_sizes = true;
const unsigned int n_tests = 1;

template <int dim, int fe_degree, int n_q_points_1d = fe_degree + 1, typename number = double>
class LaplaceOperator : public Subscriptor
{
public:
  typedef number value_type;

  LaplaceOperator(){};

  void
  clear()
  {
    data.clear();
  }

  void
  initialize(const Mapping<dim>& mapping, const DoFHandler<dim>& dof_handler,
             const unsigned int level = numbers::invalid_unsigned_int)
  {
    const QGauss<1> quad(n_q_points_1d);
    typename MatrixFree<dim, number>::AdditionalData addit_data;
    addit_data.tasks_parallel_scheme = MatrixFree<dim, number>::AdditionalData::none;
    addit_data.tasks_block_size = 3;
    addit_data.level_mg_handler = level;
#ifndef USE_FEQ
    addit_data.build_face_info = true;
#endif
    ConstraintMatrix constraints;
    constraints.close();

    data.reinit(mapping, dof_handler, constraints, quad, addit_data);

    if (level != numbers::invalid_unsigned_int)
      compute_inverse_diagonal();
  }

  void
  vmult(parallel::distributed::Vector<number>& dst,
        const parallel::distributed::Vector<number>& src) const
  {
    dst = 0;
    vmult_add(dst, src);
  }

  void
  Tvmult(parallel::distributed::Vector<number>& dst,
         const parallel::distributed::Vector<number>& src) const
  {
    dst = 0;
    vmult_add(dst, src);
  }

  void
  Tvmult_add(parallel::distributed::Vector<number>& dst,
             const parallel::distributed::Vector<number>& src) const
  {
    vmult_add(dst, src);
  }

  void
  vmult_add(parallel::distributed::Vector<number>& dst,
            const parallel::distributed::Vector<number>& src) const
  {
    Assert(src.partitioners_are_globally_compatible(*data.get_dof_info(0).vector_partitioner),
           ExcInternalError());
    Assert(dst.partitioners_are_globally_compatible(*data.get_dof_info(0).vector_partitioner),
           ExcInternalError());
    data.loop(&LaplaceOperator::local_apply,
              &LaplaceOperator::local_apply_face,
              &LaplaceOperator::local_apply_boundary,
              this,
              dst,
              src);
  }

  types::global_dof_index
  m() const
  {
    return data.get_vector_partitioner()->size();
  }

  types::global_dof_index
  n() const
  {
    return data.get_vector_partitioner()->size();
  }

  number
  el(const unsigned int, const unsigned int) const
  {
    AssertThrow(false, ExcMessage("Matrix-free does not allow for entry access"));
    return number();
  }

  void
  initialize_dof_vector(parallel::distributed::Vector<number>& vector) const
  {
    if (!vector.partitioners_are_compatible(*data.get_dof_info(0).vector_partitioner))
      data.initialize_dof_vector(vector);
  }

  const std::shared_ptr<DiagonalMatrix<parallel::distributed::Vector<number>>>
  get_matrix_diagonal_inverse() const
  {
    Assert(inverse_diagonal_entries.get() != nullptr && inverse_diagonal_entries->m() > 0,
           ExcNotInitialized());
    return inverse_diagonal_entries;
  }

private:
  void
  local_apply(const MatrixFree<dim, number>& data, parallel::distributed::Vector<number>& dst,
              const parallel::distributed::Vector<number>& src,
              const std::pair<unsigned int, unsigned int>& cell_range) const
  {
    FEEvaluation<dim, fe_degree, n_q_points_1d, 1, number> phi(data);

    for (unsigned int cell = cell_range.first; cell < cell_range.second; ++cell)
    {
      phi.reinit(cell);
      phi.read_dof_values(src);
      phi.evaluate(false, true, false);
      for (unsigned int q = 0; q < phi.n_q_points; ++q)
        phi.submit_gradient(phi.get_gradient(q), q);
      phi.integrate(false, true);
      phi.distribute_local_to_global(dst);
    }
  }

  void
  local_apply_face(const MatrixFree<dim, number>& data, parallel::distributed::Vector<number>& dst,
                   const parallel::distributed::Vector<number>& src,
                   const std::pair<unsigned int, unsigned int>& face_range) const
  {
    FEFaceEvaluation<dim, fe_degree, n_q_points_1d, 1, number> fe_eval(data, true);
    FEFaceEvaluation<dim, fe_degree, n_q_points_1d, 1, number> fe_eval_neighbor(data, false);

    for (unsigned int face = face_range.first; face < face_range.second; face++)
    {
      fe_eval.reinit(face);
      fe_eval_neighbor.reinit(face);

      fe_eval.read_dof_values(src);
      fe_eval.evaluate(true, true);
      fe_eval_neighbor.read_dof_values(src);
      fe_eval_neighbor.evaluate(true, true);
      VectorizedArray<number> sigmaF =
        (fe_eval.get_normal_volume_fraction() + fe_eval_neighbor.get_normal_volume_fraction()) *
        (number)(std::max(fe_degree, 1) * (fe_degree + 1.0)) * 0.5;

      for (unsigned int q = 0; q < fe_eval.n_q_points; ++q)
      {
        VectorizedArray<number> average_value =
          (fe_eval.get_value(q) - fe_eval_neighbor.get_value(q)) * 0.5;
        VectorizedArray<number> average_valgrad =
          fe_eval.get_normal_gradient(q) + fe_eval_neighbor.get_normal_gradient(q);
        average_valgrad = average_value * 2. * sigmaF - average_valgrad * 0.5;
        fe_eval.submit_normal_gradient(-average_value, q);
        fe_eval_neighbor.submit_normal_gradient(-average_value, q);
        fe_eval.submit_value(average_valgrad, q);
        fe_eval_neighbor.submit_value(-average_valgrad, q);
      }
      fe_eval.integrate(true, true);
      fe_eval.distribute_local_to_global(dst);
      fe_eval_neighbor.integrate(true, true);
      fe_eval_neighbor.distribute_local_to_global(dst);
    }
  }

  void
  local_apply_boundary(const MatrixFree<dim, number>& data,
                       parallel::distributed::Vector<number>& dst,
                       const parallel::distributed::Vector<number>& src,
                       const std::pair<unsigned int, unsigned int>& face_range) const
  {
    FEFaceEvaluation<dim, fe_degree, n_q_points_1d, 1, number> fe_eval(data, true);
    for (unsigned int face = face_range.first; face < face_range.second; face++)
    {
      fe_eval.reinit(face);
      fe_eval.read_dof_values(src);
      fe_eval.evaluate(true, true);
      VectorizedArray<number> sigmaF = (fe_eval.get_normal_volume_fraction()) *
                                       (number)(std::max(1, fe_degree) * (fe_degree + 1.0));

      for (unsigned int q = 0; q < fe_eval.n_q_points; ++q)
      {
        VectorizedArray<number> average_value = fe_eval.get_value(q);
        VectorizedArray<number> average_valgrad = -fe_eval.get_normal_gradient(q);
        average_valgrad += average_value * sigmaF * 2.0;
        fe_eval.submit_normal_gradient(-average_value, q);
        fe_eval.submit_value(average_valgrad, q);
      }

      fe_eval.integrate(true, true);
      fe_eval.distribute_local_to_global(dst);
    }
  }

  void
  compute_inverse_diagonal()
  {
    parallel::distributed::Vector<number>& inverse_diagonal_vector =
      inverse_diagonal_entries->get_vector();
    data.initialize_dof_vector(inverse_diagonal_vector);
    unsigned int dummy;
    data.loop(&LaplaceOperator::local_diagonal_cell,
              &LaplaceOperator::local_diagonal_face,
              &LaplaceOperator::local_diagonal_boundary,
              this,
              inverse_diagonal_vector,
              dummy);

    for (unsigned int i = 0; i < inverse_diagonal_vector.local_size(); ++i)
      if (std::abs(inverse_diagonal_vector.local_element(i)) > 1e-10)
        inverse_diagonal_vector.local_element(i) = 1. / inverse_diagonal_vector.local_element(i);
      else
        inverse_diagonal_vector.local_element(i) = 1.;
  }

  void
  local_diagonal_cell(const MatrixFree<dim, number>& data,
                      parallel::distributed::Vector<number>& dst, const unsigned int&,
                      const std::pair<unsigned int, unsigned int>& cell_range) const
  {
    FEEvaluation<dim, fe_degree, n_q_points_1d, 1, number> phi(data);

    for (unsigned int cell = cell_range.first; cell < cell_range.second; ++cell)
    {
      phi.reinit(cell);

      VectorizedArray<number> local_diagonal_vector[phi.tensor_dofs_per_cell];
      for (unsigned int i = 0; i < phi.dofs_per_cell; ++i)
      {
        for (unsigned int j = 0; j < phi.dofs_per_cell; ++j)
          phi.begin_dof_values()[j] = VectorizedArray<number>();
        phi.begin_dof_values()[i] = 1.;
        phi.evaluate(false, true, false);
        for (unsigned int q = 0; q < phi.n_q_points; ++q)
          phi.submit_gradient(phi.get_gradient(q), q);
        phi.integrate(false, true);
        local_diagonal_vector[i] = phi.begin_dof_values()[i];
      }
      for (unsigned int i = 0; i < phi.tensor_dofs_per_cell; ++i)
        phi.begin_dof_values()[i] = local_diagonal_vector[i];
      phi.distribute_local_to_global(dst);
    }
  }

  void
  local_diagonal_face(const MatrixFree<dim, number>& data,
                      parallel::distributed::Vector<number>& dst, const unsigned int&,
                      const std::pair<unsigned int, unsigned int>& face_range) const
  {
    FEFaceEvaluation<dim, fe_degree, n_q_points_1d, 1, number> phi(data, true);
    FEFaceEvaluation<dim, fe_degree, n_q_points_1d, 1, number> phi_outer(data, false);

    for (unsigned int face = face_range.first; face < face_range.second; face++)
    {
      phi.reinit(face);
      phi_outer.reinit(face);

      VectorizedArray<number> local_diagonal_vector[phi.tensor_dofs_per_cell];
      VectorizedArray<number> sigmaF =
        (phi.get_normal_volume_fraction() + phi.get_normal_volume_fraction()) *
        (number)(std::max(fe_degree, 1) * (fe_degree + 1.0)) * 0.5;

      // Compute phi part
      for (unsigned int j = 0; j < phi.dofs_per_cell; ++j)
        phi_outer.begin_dof_values()[j] = VectorizedArray<number>();
      phi_outer.evaluate(true, true);
      for (unsigned int i = 0; i < phi.dofs_per_cell; ++i)
      {
        for (unsigned int j = 0; j < phi.dofs_per_cell; ++j)
          phi.begin_dof_values()[j] = VectorizedArray<number>();
        phi.begin_dof_values()[i] = 1.;
        phi.evaluate(true, true);

        for (unsigned int q = 0; q < phi.n_q_points; ++q)
        {
          VectorizedArray<number> average_value = (phi.get_value(q) - phi_outer.get_value(q)) * 0.5;
          VectorizedArray<number> average_valgrad =
            phi.get_normal_gradient(q) + phi_outer.get_normal_gradient(q);
          average_valgrad = average_value * 2. * sigmaF - average_valgrad * 0.5;
          phi.submit_normal_gradient(-average_value, q);
          phi.submit_value(average_valgrad, q);
        }
        phi.integrate(true, true);
        local_diagonal_vector[i] = phi.begin_dof_values()[i];
      }
      for (unsigned int i = 0; i < phi.dofs_per_cell; ++i)
        phi.begin_dof_values()[i] = local_diagonal_vector[i];
      phi.distribute_local_to_global(dst);

      // Compute phi_outer part
      for (unsigned int j = 0; j < phi.dofs_per_cell; ++j)
        phi.begin_dof_values()[j] = VectorizedArray<number>();
      phi.evaluate(true, true);
      for (unsigned int i = 0; i < phi.dofs_per_cell; ++i)
      {
        for (unsigned int j = 0; j < phi.dofs_per_cell; ++j)
          phi_outer.begin_dof_values()[j] = VectorizedArray<number>();
        phi_outer.begin_dof_values()[i] = 1.;
        phi_outer.evaluate(true, true);

        for (unsigned int q = 0; q < phi.n_q_points; ++q)
        {
          VectorizedArray<number> average_value = (phi.get_value(q) - phi_outer.get_value(q)) * 0.5;
          VectorizedArray<number> average_valgrad =
            phi.get_normal_gradient(q) + phi_outer.get_normal_gradient(q);
          average_valgrad = average_value * 2. * sigmaF - average_valgrad * 0.5;
          phi_outer.submit_normal_gradient(-average_value, q);
          phi_outer.submit_value(-average_valgrad, q);
        }
        phi_outer.integrate(true, true);
        local_diagonal_vector[i] = phi_outer.begin_dof_values()[i];
      }
      for (unsigned int i = 0; i < phi.dofs_per_cell; ++i)
        phi_outer.begin_dof_values()[i] = local_diagonal_vector[i];
      phi_outer.distribute_local_to_global(dst);
    }
  }

  void
  local_diagonal_boundary(const MatrixFree<dim, number>& data,
                          parallel::distributed::Vector<number>& dst, const unsigned int&,
                          const std::pair<unsigned int, unsigned int>& face_range) const
  {
    FEFaceEvaluation<dim, fe_degree, n_q_points_1d, 1, number> phi(data);

    for (unsigned int face = face_range.first; face < face_range.second; face++)
    {
      phi.reinit(face);

      VectorizedArray<number> local_diagonal_vector[phi.tensor_dofs_per_cell];
      VectorizedArray<number> sigmaF =
        (phi.get_normal_volume_fraction()) * (number)(std::max(1, fe_degree) * (fe_degree + 1.0));

      for (unsigned int i = 0; i < phi.dofs_per_cell; ++i)
      {
        for (unsigned int j = 0; j < phi.dofs_per_cell; ++j)
          phi.begin_dof_values()[j] = VectorizedArray<number>();
        phi.begin_dof_values()[i] = 1.;
        phi.evaluate(true, true);

        for (unsigned int q = 0; q < phi.n_q_points; ++q)
        {
          VectorizedArray<number> average_value = phi.get_value(q);
          VectorizedArray<number> average_valgrad = -phi.get_normal_gradient(q);
          average_valgrad += average_value * sigmaF * 2.0;
          phi.submit_normal_gradient(-average_value, q);
          phi.submit_value(average_valgrad, q);
        }

        phi.integrate(true, true);
        local_diagonal_vector[i] = phi.begin_dof_values()[i];
      }
      for (unsigned int i = 0; i < phi.dofs_per_cell; ++i)
        phi.begin_dof_values()[i] = local_diagonal_vector[i];
      phi.distribute_local_to_global(dst);
    }
  }

  MatrixFree<dim, number> data;
  std::shared_ptr<DiagonalMatrix<parallel::distributed::Vector<number>>> inverse_diagonal_entries;
};

template <int dim, typename LAPLACEOPERATOR>
class MGTransferMF : public MGTransferMatrixFree<dim, typename LAPLACEOPERATOR::value_type>
{
public:
  MGTransferMF(const MGLevelObject<LAPLACEOPERATOR>& laplace)
    : laplace_operator(laplace){};

  /**
   * Overload copy_to_mg from MGTransferPrebuilt
   */
  template <class InVector, int spacedim>
  void
  copy_to_mg(
    const DoFHandler<dim, spacedim>& mg_dof,
    MGLevelObject<parallel::distributed::Vector<typename LAPLACEOPERATOR::value_type>>& dst,
    const InVector& src) const
  {
    for (unsigned int level = dst.min_level(); level <= dst.max_level(); ++level)
      laplace_operator[level].initialize_dof_vector(dst[level]);
    MGLevelGlobalTransfer<
      parallel::distributed::Vector<typename LAPLACEOPERATOR::value_type>>::copy_to_mg(mg_dof,
                                                                                       dst,
                                                                                       src);
  }

private:
  const MGLevelObject<LAPLACEOPERATOR>& laplace_operator;
};

template <typename MATRIX, typename Number>
class MGCoarseIterative : public MGCoarseGridBase<parallel::distributed::Vector<Number>>
{
public:
  MGCoarseIterative() {}

  void
  initialize(const MATRIX& matrix)
  {
    coarse_matrix = &matrix;
  }

  virtual void
  operator()(const unsigned int, parallel::distributed::Vector<double>& dst,
             const parallel::distributed::Vector<double>& src) const
  {
    ReductionControl solver_control(1e4, 1e-50, 1e-10);
    SolverCG<parallel::distributed::Vector<double>> solver_coarse(solver_control);
    solver_coarse.solve(*coarse_matrix, dst, src, PreconditionIdentity());
  }

  const MATRIX* coarse_matrix;
};

template <typename VECTOR>
class MGCoarseFromSmoother : public MGCoarseGridBase<VECTOR>
{
public:
  MGCoarseFromSmoother() {}

  void
  initialize(const MGSmootherBase<VECTOR>& mg_smoother)
  {
    smoother = &mg_smoother;
  }

  virtual void
  operator()(const unsigned int level, VECTOR& dst, const VECTOR& src) const
  {
    Assert(level == 0, ExcNotImplemented());
    smoother->smooth(level, dst, src);
  }

  const MGSmootherBase<VECTOR>* smoother;
};

template <int dim>
class MatrixIntegrator : public MeshWorker::LocalIntegrator<dim>
{
public:
  void cell(MeshWorker::DoFInfo<dim>& dinfo, typename MeshWorker::IntegrationInfo<dim>& info) const;
  void boundary(MeshWorker::DoFInfo<dim>& dinfo,
                typename MeshWorker::IntegrationInfo<dim>& info) const;
  void face(MeshWorker::DoFInfo<dim>& dinfo1, MeshWorker::DoFInfo<dim>& dinfo2,
            typename MeshWorker::IntegrationInfo<dim>& info1,
            typename MeshWorker::IntegrationInfo<dim>& info2) const;
};

template <int dim>
void
MatrixIntegrator<dim>::cell(MeshWorker::DoFInfo<dim>& dinfo,
                            typename MeshWorker::IntegrationInfo<dim>& info) const
{
  // return;
  LocalIntegrators::Laplace::cell_matrix(dinfo.matrix(0, false).matrix, info.fe_values());
}

// Interior faces use the interior penalty method
template <int dim>
void
MatrixIntegrator<dim>::face(MeshWorker::DoFInfo<dim>& dinfo1, MeshWorker::DoFInfo<dim>& dinfo2,
                            typename MeshWorker::IntegrationInfo<dim>& info1,
                            typename MeshWorker::IntegrationInfo<dim>& info2) const
{
  // return;
  const unsigned int deg = info1.fe_values(0).get_fe().tensor_degree();
  LocalIntegrators::Laplace::ip_matrix(
    dinfo1.matrix(0, false).matrix,
    dinfo1.matrix(0, true).matrix,
    dinfo2.matrix(0, true).matrix,
    dinfo2.matrix(0, false).matrix,
    info1.fe_values(0),
    info2.fe_values(0),
    LocalIntegrators::Laplace::compute_penalty(dinfo1, dinfo2, deg, deg));
}

template <int dim>
void
MatrixIntegrator<dim>::boundary(MeshWorker::DoFInfo<dim>& dinfo,
                                typename MeshWorker::IntegrationInfo<dim>& info) const
{
  // return;
  const unsigned int deg = info.fe_values(0).get_fe().tensor_degree();
  LocalIntegrators::Laplace::nitsche_matrix(
    dinfo.matrix(0, false).matrix,
    info.fe_values(0),
    LocalIntegrators::Laplace::compute_penalty(dinfo, dinfo, deg, deg));
}

template <int dim>
class LaplaceProblem
{
public:
  typedef float Number;

  LaplaceProblem();
  void run();

private:
  void setup_system();
  void assemble_system();
  void solve();
  void output_results(const unsigned int cycle) const;

  typedef LaplaceOperator<dim, degree_finite_element, degree_finite_element + 1, double>
    SystemMatrixType;

  parallel::distributed::Triangulation<dim> triangulation;
#ifndef USE_FEQ
  FE_DGQArbitraryNodes<dim> fe;
#else
  FE_Q<dim> fe;
#endif
  MappingQ<dim> mapping;
  DoFHandler<dim> dof_handler;
  SystemMatrixType system_matrix;

  parallel::distributed::Vector<double> solution;
  parallel::distributed::Vector<double> solution_update;
  parallel::distributed::Vector<double> system_rhs;

  double setup_time;
  mutable ConditionalOStream pcout;
  ConditionalOStream time_details;

  ConvergenceTable convergence_table;
};

template <int dim>
LaplaceProblem<dim>::LaplaceProblem()
  : triangulation(MPI_COMM_WORLD, Triangulation<dim>::none,
                  parallel::distributed::Triangulation<dim>::construct_multigrid_hierarchy)
  , fe(QGaussLobatto<1>(degree_finite_element + 1))
  , mapping(degree_finite_element)
  , dof_handler(triangulation)
  , pcout(std::cout, Utilities::MPI::this_mpi_process(MPI_COMM_WORLD) == 0)
  , time_details(std::cout, true && Utilities::MPI::this_mpi_process(MPI_COMM_WORLD) == 0)
{
}

template <int dim>
void
LaplaceProblem<dim>::setup_system()
{
  Timer time;
  time.start();
  setup_time = 0;

  system_matrix.clear();

  dof_handler.distribute_dofs(fe);
  dof_handler.distribute_mg_dofs(fe);

  pcout << "Number of active cells:       " << triangulation.n_global_active_cells() << std::endl;
  pcout << "Number of degrees of freedom: " << dof_handler.n_dofs() << std::endl;

  setup_time += time.wall_time();
  time_details << "Distribute DoFs & B.C.     (CPU/wall) " << time() << "s/" << time.wall_time()
               << "s" << std::endl;
  time.restart();

  system_matrix.initialize(mapping, dof_handler, numbers::invalid_unsigned_int);
  system_matrix.initialize_dof_vector(system_rhs);
  solution.reinit(system_rhs);
  solution_update.reinit(system_rhs);
  for (unsigned int i = 0; i < system_rhs.local_size(); ++i)
    system_rhs.local_element(i) = (double)rand() / RAND_MAX;

  setup_time += time.wall_time();
  time_details << "Setup matrix-free system   (CPU/wall) " << time() << "s/" << time.wall_time()
               << "s" << std::endl;
}

template <int dim>
void
LaplaceProblem<dim>::assemble_system()
{
  Timer time;
  std::map<types::global_dof_index, double> boundary_values;
  VectorTools::interpolate_boundary_values(dof_handler, 0, Solution<dim>(), boundary_values);
  for (typename std::map<types::global_dof_index, double>::const_iterator it =
         boundary_values.begin();
       it != boundary_values.end();
       ++it)
    solution[it->first] = it->second;

  system_rhs = 0;
  QGauss<dim> quadrature_formula(fe.degree + 1);
  QGauss<dim - 1> quadrature_face(fe.degree + 1);
  FEValues<dim> fe_values(fe,
                          quadrature_formula,
                          update_values | update_gradients | update_JxW_values |
                            update_quadrature_points);
  FEFaceValues<dim> fe_face_values(fe,
                                   quadrature_face,
                                   update_values | update_gradients | update_quadrature_points |
                                     update_normal_vectors);

  const unsigned int dofs_per_cell = fe.dofs_per_cell;
  const unsigned int n_q_points = quadrature_formula.size();

  std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);
  RightHandSide<dim> right_hand_side;
  std::vector<double> rhs_values(n_q_points);
  Solution<dim> solution_function;
  std::vector<Tensor<1, dim>> solution_gradients(n_q_points);

  typename DoFHandler<dim>::active_cell_iterator cell = dof_handler.begin_active(),
                                                 endc = dof_handler.end();
  for (; cell != endc; ++cell)
    if (cell->is_locally_owned())
    {
      cell->get_dof_indices(local_dof_indices);
      fe_values.reinit(cell);
#ifdef USE_FEQ
      fe_values.get_function_gradients(solution, solution_gradients);
#endif
      right_hand_side.value_list(fe_values.get_quadrature_points(), rhs_values);
      for (unsigned int i = 0; i < dofs_per_cell; ++i)
      {
        double rhs_val = 0;
        for (unsigned int q = 0; q < n_q_points; ++q)
          rhs_val += ((fe_values.shape_value(i, q) * rhs_values[q]
#ifdef USE_FEQ
                       -
                       fe_values.shape_grad(i, q) * solution_gradients[q]
#endif
                       ) *
                      fe_values.JxW(q));
        system_rhs(local_dof_indices[i]) += rhs_val;
      }
#ifndef USE_FEQ
      for (unsigned int face = 0; face < GeometryInfo<dim>::faces_per_cell; ++face)
        if (cell->at_boundary(face))
        {
          double cell_measure = 0;
          for (unsigned int q = 0; q < n_q_points; ++q)
            cell_measure += fe_values.JxW(q);
          fe_face_values.reinit(cell, face);
          double face_measure = 0;
          for (unsigned int q = 0; q < quadrature_face.size(); ++q)
            face_measure += fe_face_values.JxW(q);
          const double sigmaF = fe.degree * (fe.degree + 1.) * face_measure / cell_measure * 2.0;
          for (unsigned int q = 0; q < quadrature_face.size(); ++q)
          {
            const double solution_value =
              solution_function.value(fe_face_values.quadrature_point(q));
            for (unsigned int i = 0; i < dofs_per_cell; ++i)
              system_rhs(local_dof_indices[i]) +=
                (sigmaF * fe_face_values.shape_value(i, q) -
                 fe_face_values.shape_grad(i, q) * fe_face_values.normal_vector(q)) *
                solution_value * fe_face_values.JxW(q);
          }
        }
#endif
    }

  setup_time += time.wall_time();
  time_details << "Assemble right hand side   (CPU/wall) " << time() << "s/" << time.wall_time()
               << "s" << std::endl;
}

template <int dim>
void
LaplaceProblem<dim>::solve()
{
  Timer time;
  typedef LaplaceOperator<dim, degree_finite_element, degree_finite_element + 1, Number>
    LevelMatrixType;

  MGLevelObject<LevelMatrixType> mg_matrices;
  mg_matrices.resize(0, dof_handler.get_triangulation().n_global_levels() - 1);
  for (unsigned int level = 0; level < triangulation.n_global_levels(); ++level)
  {
    mg_matrices[level].initialize(mapping, dof_handler, level);
  }

  time_details << "Setup matrix-free levels   (CPU/wall) " << time() << "s/" << time.wall_time()
               << "s" << std::endl;
  setup_time += time.wall_time();
  time.restart();
  {
    Utilities::System::MemoryStats stats;
    Utilities::System::get_memory_stats(stats);
    Utilities::MPI::MinMaxAvg memory =
      Utilities::MPI::min_max_avg(stats.VmRSS / 1024., triangulation.get_communicator());
    pcout << "Memory stats [MB]: " << memory.min << " [p" << memory.min_index << "] " << memory.avg
          << " " << memory.max << " [p" << memory.max_index << "]" << std::endl;
  }

  // MGTransferPrebuilt<parallel::distributed::Vector<Number> > mg_transfer;
  MGTransferMF<dim, LevelMatrixType> mg_transfer(mg_matrices);
  mg_transfer.build(dof_handler);
  {
    Utilities::System::MemoryStats stats;
    Utilities::System::get_memory_stats(stats);
    Utilities::MPI::MinMaxAvg memory =
      Utilities::MPI::min_max_avg(stats.VmRSS / 1024., triangulation.get_communicator());
    pcout << "Memory stats [MB]: " << memory.min << " [p" << memory.min_index << "] " << memory.avg
          << " " << memory.max << " [p" << memory.max_index << "]" << std::endl;
  }

  setup_time += time.wall_time();
  time_details << "MG build transfer time     (CPU/wall) " << time() << "s/" << time.wall_time()
               << "s\n";
  time.restart();

  typedef PreconditionChebyshev<LevelMatrixType, parallel::distributed::Vector<Number>> SMOOTHER;
  MGSmootherPrecondition<LevelMatrixType, SMOOTHER, parallel::distributed::Vector<Number>>
    mg_smoother;

  MGLevelObject<typename SMOOTHER::AdditionalData> smoother_data;
  smoother_data.resize(0, dof_handler.get_triangulation().n_global_levels() - 1);
  for (unsigned int level = 0; level < dof_handler.get_triangulation().n_global_levels(); ++level)
  {
    if (level > 0)
    {
      smoother_data[level].smoothing_range = 15.;
      smoother_data[level].degree = 5;
      smoother_data[level].eig_cg_n_iterations = 15;
    }
    else
    {
      smoother_data[level].smoothing_range = 0.;
      if (dof_handler.n_dofs(0) > 2000)
      {
        smoother_data[level].degree = 100;
        smoother_data[level].eig_cg_n_iterations = 200;
      }
      else
      {
        smoother_data[level].degree = 40;
        smoother_data[level].eig_cg_n_iterations = 100;
      }
    }
    smoother_data[level].preconditioner = mg_matrices[level].get_matrix_diagonal_inverse();
  }
  mg_smoother.initialize(mg_matrices, smoother_data);

  SolverControl solver_control(10000, 1e-12 * system_rhs.l2_norm());
  SolverCG<parallel::distributed::Vector<double>> cg(solver_control);
  setup_time += time.wall_time();
  time_details << "MG build smoother time     (CPU/wall) " << time() << "s/" << time.wall_time()
               << "s\n";
  time.restart();

  MGCoarseFromSmoother<parallel::distributed::Vector<Number>> mg_coarse;
  mg_coarse.initialize(mg_smoother);
  // MGCoarseIterative<LevelMatrixType,Number> mg_coarse;
  // mg_coarse.initialize(mg_matrices[0]);

  time_details << "MG coarse time             (CPU/wall) " << time() << "s/" << time.wall_time()
               << "s\n";
  setup_time += time.wall_time();
  time.restart();

  mg::Matrix<parallel::distributed::Vector<Number>> mg_matrix(mg_matrices);

  Multigrid<parallel::distributed::Vector<Number>> mg(
    mg_matrix, mg_coarse, mg_transfer, mg_smoother, mg_smoother);
  PreconditionMG<dim, parallel::distributed::Vector<Number>, MGTransferMF<dim, LevelMatrixType>>
    // MGTransferPrebuilt<parallel::distributed::Vector<double> > >
    preconditioner(dof_handler, mg, mg_transfer);
  setup_time += time.wall_time();

  pcout << "Total setup time               (wall) " << setup_time << "s\n";
  pcout << "Number of multigrid levels: " << triangulation.n_global_levels() << std::endl;

  double sol_time = 1e10;
  for (unsigned int i = 0; i < n_tests; ++i)
  {
    solution = 0;
    time.restart();

    cg.solve(system_matrix, solution, system_rhs, preconditioner);

    sol_time = std::min(sol_time, time.wall_time());
    pcout << "Time solve (" << solver_control.last_step() << " iterations)  (CPU/wall) " << time()
          << "s/" << time.wall_time() << "s\n";
  }
  {
    Utilities::System::MemoryStats stats;
    Utilities::System::get_memory_stats(stats);
    Utilities::MPI::MinMaxAvg memory =
      Utilities::MPI::min_max_avg(stats.VmRSS / 1024., triangulation.get_communicator());
    pcout << "Memory stats [MB]: " << memory.min << " [p" << memory.min_index << "] " << memory.avg
          << " " << memory.max << " [p" << memory.max_index << "]" << std::endl;
  }

  if (false && Utilities::MPI::n_mpi_processes(MPI_COMM_WORLD) == 1 &&
      dof_handler.n_dofs() * fe.dofs_per_cell < 1000000)
  {
    // mesh worker for computing reference
    TrilinosWrappers::SparseMatrix matrix;
    {
      {
        TrilinosWrappers::SparsityPattern sparsity;
        DynamicSparsityPattern d_sparsity(dof_handler.n_dofs());
        DoFTools::make_flux_sparsity_pattern(dof_handler, d_sparsity);
        sparsity.reinit(dof_handler.locally_owned_dofs(), d_sparsity, MPI_COMM_WORLD);
        matrix.reinit(sparsity);
      }
      MeshWorker::IntegrationInfoBox<dim> info_box;
      UpdateFlags update_flags = update_values | update_gradients;
      info_box.add_update_flags_all(update_flags);
      info_box.initialize(fe, mapping);
      MeshWorker::DoFInfo<dim> dof_info(dof_handler);

      MeshWorker::Assembler::MatrixSimple<TrilinosWrappers::SparseMatrix> assembler;
      assembler.initialize(matrix);
      MatrixIntegrator<dim> integrator;
      MeshWorker::integration_loop<dim, dim>(
        dof_handler.begin_active(), dof_handler.end(), dof_info, info_box, integrator, assembler);
      matrix.compress(VectorOperation::add);
    }

    MGLevelObject<TrilinosWrappers::SparseMatrix> mg_matrix;
    MGTransferPrebuilt<parallel::distributed::Vector<double>> mg_transfer;
    const unsigned int n_levels = triangulation.n_levels();
    mg_matrix.resize(0, n_levels - 1);
    mg_matrix.clear_elements();
    for (unsigned int level = mg_matrix.min_level(); level <= mg_matrix.max_level(); ++level)
    {
      DynamicSparsityPattern c_sparsity(dof_handler.n_dofs(level));
      MGTools::make_flux_sparsity_pattern(dof_handler, c_sparsity, level);
      mg_matrix[level].reinit(dof_handler.locally_owned_mg_dofs(level), c_sparsity, MPI_COMM_WORLD);
    }
    {
      MeshWorker::IntegrationInfoBox<dim> info_box;
      UpdateFlags update_flags = update_values | update_gradients;
      info_box.add_update_flags_all(update_flags);
      info_box.initialize(fe, mapping);
      MeshWorker::DoFInfo<dim> dof_info(dof_handler);
      MeshWorker::Assembler::MGMatrixSimple<TrilinosWrappers::SparseMatrix> assembler;
      assembler.initialize(mg_matrix);
      MatrixIntegrator<dim> integrator;
      MeshWorker::integration_loop<dim, dim>(
        dof_handler.begin_mg(), dof_handler.end_mg(), dof_info, info_box, integrator, assembler);

      mg_transfer.build_matrices(dof_handler);
    }
    MGCoarseIterative<TrilinosWrappers::SparseMatrix, double> mg_coarse;
    mg_coarse.initialize(mg_matrix[0]);

    MGSmootherPrecondition<TrilinosWrappers::SparseMatrix,
                           PreconditionChebyshev<TrilinosWrappers::SparseMatrix,
                                                 parallel::distributed::Vector<
                                                   double>> /*TrilinosWrappers::PreconditionILU*/,
                           parallel::distributed::Vector<double>>
      mg_smoother;
    PreconditionChebyshev<TrilinosWrappers::SparseMatrix,
                          parallel::distributed::Vector<double>>::AdditionalData smoother_data;
    smoother_data.smoothing_range = 15;
    smoother_data.degree = 4;
    smoother_data.eig_cg_n_iterations = 10;
    // TrilinosWrappers::PreconditionILU::AdditionalData smoother_data;
    mg_smoother.initialize(mg_matrix, smoother_data);

    mg::Matrix<parallel::distributed::Vector<double>> mgmatrix(mg_matrix);
    Multigrid<parallel::distributed::Vector<double>> mg(
      mgmatrix, mg_coarse, mg_transfer, mg_smoother, mg_smoother);
    PreconditionMG<dim,
                   parallel::distributed::Vector<double>,
                   MGTransferPrebuilt<parallel::distributed::Vector<double>>>
      preconditioner(dof_handler, mg, mg_transfer);

    SolverControl solver_control(10000, 1e-12 * system_rhs.l2_norm());
    SolverCG<parallel::distributed::Vector<double>> cg(solver_control);

    cg.solve(matrix, solution_update, system_rhs, preconditioner);
    pcout << "Number of CG iterations with matrix: " << solver_control.last_step() << std::endl;
  }

  Vector<float> difference_per_cell(triangulation.n_active_cells());
  VectorTools::integrate_difference(dof_handler,
                                    solution,
                                    Solution<dim>(),
                                    difference_per_cell,
                                    QGauss<dim>(fe.degree + 2),
                                    VectorTools::L2_norm);
  const double L2_error =
    std::sqrt(Utilities::MPI::sum(difference_per_cell.norm_sqr(), MPI_COMM_WORLD));

  std::cout.precision(6);
  pcout << "L2 error: " << L2_error << std::endl;
  VectorTools::integrate_difference(dof_handler,
                                    solution,
                                    Solution<dim>(),
                                    difference_per_cell,
                                    QGauss<dim>(fe.degree + 2),
                                    VectorTools::H1_seminorm);
  const double H1_error =
    std::sqrt(Utilities::MPI::sum(difference_per_cell.norm_sqr(), MPI_COMM_WORLD));

  const unsigned int n_active_cells = triangulation.n_global_active_cells();
  const unsigned int n_dofs = dof_handler.n_dofs();

  convergence_table.add_value("cycle", triangulation.n_global_levels() - 1);
  convergence_table.add_value("cells", n_active_cells);
  convergence_table.add_value("dofs", n_dofs);
  convergence_table.add_value("L2", L2_error);
  convergence_table.add_value("H1", H1_error);
  convergence_table.add_value("Sol time", sol_time);
}

template <int dim>
void
LaplaceProblem<dim>::output_results(const unsigned int cycle) const
{
  return;
  DataOut<dim> data_out;

  data_out.attach_dof_handler(dof_handler);
  data_out.add_data_vector(solution, "solution");
  data_out.build_patches();

  std::ostringstream filename;
  filename << "solution-" << cycle << ".vtu";

  std::ofstream output(filename.str().c_str());
  data_out.write_vtu(output);
}

template <int dim>
void
LaplaceProblem<dim>::run()
{
  pcout << "Number of MPI processes: " << Utilities::MPI::n_mpi_processes(MPI_COMM_WORLD)
        << std::endl;
  pcout << "Testing " << fe.get_name() << std::endl << std::endl;

  unsigned int n_cycles = 5;
  unsigned int sizes[] = { 1,  2,  3,  4,  5,  6,  7,  8,  10, 12, 14,  16,
                           20, 24, 28, 32, 40, 48, 56, 64, 80, 96, 112, 128 };
  if (run_variable_sizes)
    n_cycles = sizeof(sizes) / sizeof(unsigned int);
  for (unsigned int cycle = 0; cycle < n_cycles; ++cycle)
  {
    pcout << "Cycle " << cycle << std::endl;

    if (run_variable_sizes)
    {
      triangulation.clear();
      unsigned int n_refinements = 0;
      unsigned int n_subdiv = sizes[cycle];
      if (n_subdiv > 1)
        while (n_subdiv % 2 == 0)
        {
          n_refinements += 1;
          n_subdiv /= 2;
        }
      if (dim == 2)
        n_refinements += 3;
      if (Utilities::MPI::n_mpi_processes(MPI_COMM_WORLD) > 64)
        n_refinements += 6 - dim;
      if (Utilities::MPI::n_mpi_processes(MPI_COMM_WORLD) > 512)
        n_refinements += 4 - dim;
      if (Utilities::MPI::n_mpi_processes(MPI_COMM_WORLD) > 2048)
        n_refinements += 1;
      if (Utilities::MPI::n_mpi_processes(MPI_COMM_WORLD) > 4096)
        n_refinements += 1;
      GridGenerator::subdivided_hyper_cube(triangulation, n_subdiv, -1, 1);
      triangulation.refine_global(n_refinements);
    }
    else
    {
      if (cycle == 0)
      {
        GridGenerator::hyper_cube(triangulation, -1, 1.);
        // triangulation.refine_global (3-dim);
      }
      triangulation.refine_global(1);
      // triangulation.begin_active()->set_refine_flag();
      // triangulation.execute_coarsening_and_refinement();
    }
    setup_system();

    assemble_system();
    pcout << "Solve\n";
    solve();
    output_results(cycle);
    pcout << std::endl;
    if (dof_handler.n_dofs() >
        types::global_dof_index(8000000) * Utilities::MPI::n_mpi_processes(MPI_COMM_WORLD))
      break;
  };

  convergence_table.set_precision("L2", 3);
  convergence_table.set_precision("H1", 3);

  convergence_table.set_scientific("L2", true);
  convergence_table.set_scientific("H1", true);

  convergence_table.evaluate_convergence_rates(
    "L2", "cells", ConvergenceTable::reduction_rate_log2, dim);
  convergence_table.evaluate_convergence_rates(
    "H1", "cells", ConvergenceTable::reduction_rate_log2, dim);
  if (Utilities::MPI::this_mpi_process(MPI_COMM_WORLD) == 0)
    convergence_table.write_text(std::cout);
}
}

int
main(int argc, char** argv)
{
  try
  {
    using namespace Step37;
    Utilities::MPI::MPI_InitFinalize mpi(argc, argv, 1);

    deallog.depth_console(0);
    LaplaceProblem<dimension> laplace_problem;
    laplace_problem.run();
  }
  catch (std::exception& exc)
  {
    std::cerr << std::endl
              << std::endl
              << "----------------------------------------------------" << std::endl;
    std::cerr << "Exception on processing: " << std::endl
              << exc.what() << std::endl
              << "Aborting!" << std::endl
              << "----------------------------------------------------" << std::endl;
    return 1;
  }
  catch (...)
  {
    std::cerr << std::endl
              << std::endl
              << "----------------------------------------------------" << std::endl;
    std::cerr << "Unknown exception!" << std::endl
              << "Aborting!" << std::endl
              << "----------------------------------------------------" << std::endl;
    return 1;
  }

  return 0;
}
